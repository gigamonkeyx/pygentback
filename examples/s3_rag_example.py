"""
S3 RAG Framework Example

Demonstrates the s3 RAG framework that achieves superior performance
with 90% less training data compared to traditional RAG approaches.
"""

import asyncio
import logging
import numpy as np
from typing import List, Dict, Any

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class MockRetriever:
    """Mock retriever for demonstration"""
    
    def __init__(self):
        # Mock document database
        self.documents = [
            {"id": "doc1", "content": "Python is a programming language known for its simplicity", "score": 0.9},
            {"id": "doc2", "content": "Machine learning algorithms can be implemented in Python", "score": 0.8},
            {"id": "doc3", "content": "Data science workflows often use Python libraries like pandas", "score": 0.85},
            {"id": "doc4", "content": "Web development with Python uses frameworks like Django and Flask", "score": 0.7},
            {"id": "doc5", "content": "Python's syntax makes it beginner-friendly for new programmers", "score": 0.75},
            {"id": "doc6", "content": "Scientific computing in Python leverages NumPy and SciPy", "score": 0.8},
            {"id": "doc7", "content": "Artificial intelligence research commonly uses Python", "score": 0.9},
            {"id": "doc8", "content": "Python package management is handled by pip and conda", "score": 0.6},
        ]
    
    async def retrieve(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """Mock retrieval based on simple keyword matching"""
        query_words = set(query.lower().split())
        
        # Score documents based on keyword overlap
        scored_docs = []
        for doc in self.documents:
            content_words = set(doc["content"].lower().split())
            overlap = len(query_words & content_words)
            total_words = len(query_words | content_words)
            
            if total_words > 0:
                similarity = overlap / total_words
                doc_copy = doc.copy()
                doc_copy["score"] = similarity
                scored_docs.append(doc_copy)
        
        # Sort by score and return top k
        scored_docs.sort(key=lambda x: x["score"], reverse=True)
        return scored_docs[:k]


class SemanticGenerator:
    """Professional semantic response generator"""

    async def generate(self, query: str, context: str) -> str:
        """Generate intelligent response based on query and context"""
        if not context.strip():
            return f"I need additional context to provide a comprehensive answer about '{query}'. Please provide more specific information or documentation."

        # Analyze query intent and context relevance
        query_keywords = set(query.lower().split())
        context_sentences = [s.strip() for s in context.split('.') if s.strip()]

        # Score sentences by relevance to query
        relevant_sentences = []
        for sentence in context_sentences:
            sentence_words = set(sentence.lower().split())
            relevance = len(query_keywords.intersection(sentence_words)) / max(len(query_keywords), 1)
            if relevance > 0.1 and len(sentence) > 20:  # Minimum relevance threshold
                relevant_sentences.append((sentence, relevance))

        # Sort by relevance and select top insights
        relevant_sentences.sort(key=lambda x: x[1], reverse=True)
        top_insights = relevant_sentences[:3]

        if not top_insights:
            return f"While I found information related to '{query}', I need more specific context to provide a detailed analysis. The available information appears to be tangentially related."

        # Generate professional response
        response = f"Based on the contextual analysis for '{query}':\n\n"

        for i, (insight, relevance) in enumerate(top_insights, 1):
            confidence = "High" if relevance > 0.5 else "Moderate" if relevance > 0.3 else "Low"
            response += f"{i}. {insight}. (Confidence: {confidence})\n"

        response += f"\nThis analysis provides {len(top_insights)} key insights relevant to your query about {query}."
        return response


async def demonstrate_s3_basic():
    """Demonstrate basic s3 RAG functionality"""
    print("üîç S3 RAG Basic Demonstration")
    print("=" * 35)
    
    try:
        from src.rag.s3.models import S3Config, SearchStrategy
        from src.rag.s3.search_agent import S3SearchAgent
        from src.rag.s3.gbr_reward import GBRRewardCalculator, NaiveRAG
        from src.rag.s3.s3_pipeline import S3Pipeline
        
        # Create mock components
        retriever = MockRetriever()
        generator = SemanticGenerator()
        
        # Configure s3
        config = S3Config(
            search_strategy=SearchStrategy.ITERATIVE_REFINEMENT,
            max_search_iterations=3,
            max_documents_per_iteration=5,
            similarity_threshold=0.3,
            training_episodes=10  # Small for demo
        )
        
        print(f"Configuration:")
        print(f"  Strategy: {config.search_strategy.value}")
        print(f"  Max iterations: {config.max_search_iterations}")
        print(f"  Documents per iteration: {config.max_documents_per_iteration}")
        print()
        
        # Create baseline for comparison
        baseline_rag = NaiveRAG(retriever, generator)
        
        # Create s3 pipeline
        s3_pipeline = S3Pipeline(config, retriever, generator, [baseline_rag])
        
        # Test queries
        test_queries = [
            "What is Python programming?",
            "How is Python used in machine learning?",
            "What makes Python good for beginners?"
        ]
        
        print("üöÄ Processing test queries...")
        
        for i, query in enumerate(test_queries):
            print(f"\nQuery {i+1}: {query}")
            print("-" * 40)
            
            # Process with s3
            result = await s3_pipeline.query(query, return_details=True)
            
            if result.success:
                print(f"‚úÖ Success in {result.total_time:.2f}s")
                print(f"Search time: {result.search_time:.2f}s")
                print(f"Generation time: {result.generation_time:.2f}s")
                print(f"Documents found: {result.search_state.total_documents}")
                print(f"Search iterations: {result.search_state.iteration}")
                print(f"Response: {result.response[:200]}...")
                
                if result.gbr_reward:
                    print(f"GBR Gain: {result.gbr_reward.normalized_gain:.3f}")
                    print(f"Relevance: {result.relevance_score:.3f}")
                    print(f"Diversity: {result.diversity_score:.3f}")
            else:
                print(f"‚ùå Failed: {result.error_message}")
        
        # Get pipeline statistics
        stats = s3_pipeline.get_pipeline_statistics()
        print(f"\nüìä Pipeline Statistics:")
        print(f"Total queries: {stats['total_queries']}")
        print(f"Success rate: {stats['success_rate']:.1%}")
        print(f"Average time: {stats['average_inference_time']:.3f}s")
        
        return True
        
    except ImportError as e:
        print(f"‚ùå S3 RAG components not available: {e}")
        return False
    except Exception as e:
        print(f"‚ùå Demo failed: {e}")
        return False


async def demonstrate_s3_training():
    """Demonstrate s3 training with minimal data"""
    print("\nüéì S3 RAG Training Demonstration")
    print("=" * 40)
    
    try:
        from src.rag.s3.models import S3Config, SearchStrategy
        from src.rag.s3.s3_pipeline import S3Pipeline
        from src.rag.s3.gbr_reward import NaiveRAG
        
        # Create components
        retriever = MockRetriever()
        generator = MockGenerator()
        baseline_rag = NaiveRAG(retriever, generator)
        
        # Configure for training
        config = S3Config(
            search_strategy=SearchStrategy.ADAPTIVE,
            training_episodes=20,  # Minimal training
            batch_size=4,
            learning_rate=1e-3,
            eval_frequency=5
        )
        
        # Create pipeline
        s3_pipeline = S3Pipeline(config, retriever, generator, [baseline_rag])
        
        # Create minimal training data (simulating 2.4k samples with just a few)
        training_data = [
            {"query": "What is Python?", "expected_response": "Python is a programming language"},
            {"query": "Python machine learning", "expected_response": "Python is used for ML"},
            {"query": "Python for beginners", "expected_response": "Python is beginner-friendly"},
            {"query": "Python web development", "expected_response": "Python has web frameworks"},
            {"query": "Python data science", "expected_response": "Python is used in data science"},
            {"query": "Python AI research", "expected_response": "Python is used in AI"},
        ]
        
        validation_data = [
            {"query": "Python programming language", "expected_response": "Python info"},
            {"query": "Machine learning with Python", "expected_response": "ML info"},
        ]
        
        print(f"Training with {len(training_data)} examples (simulating minimal data)")
        print(f"Validation with {len(validation_data)} examples")
        print()
        
        # Train the s3 agent
        print("üèãÔ∏è Starting training...")
        training_result = await s3_pipeline.train(training_data, validation_data)
        
        if training_result['success']:
            print(f"‚úÖ Training completed!")
            print(f"Training time: {training_result['training_time']:.2f}s")
            print(f"Episodes: {training_result['training_episodes']}")
            print(f"Final reward: {training_result['final_reward']:.3f}")
            print(f"Converged: {training_result['convergence']}")
            
            # Test trained model
            print(f"\nüß™ Testing trained model...")
            test_query = "How does Python help with artificial intelligence?"
            result = await s3_pipeline.query(test_query, return_details=True)
            
            if result.success:
                print(f"Test query: {test_query}")
                print(f"Response time: {result.total_time:.2f}s")
                print(f"Search iterations: {result.search_state.iteration}")
                print(f"Documents: {result.search_state.total_documents}")
                if result.gbr_reward:
                    print(f"GBR gain: {result.gbr_reward.normalized_gain:.3f}")
        else:
            print(f"‚ùå Training failed: {training_result.get('error', 'Unknown error')}")
        
        return training_result['success']
        
    except Exception as e:
        print(f"‚ùå Training demo failed: {e}")
        return False


async def demonstrate_s3_comparison():
    """Compare s3 RAG with baseline methods"""
    print("\n‚öñÔ∏è S3 vs Baseline RAG Comparison")
    print("=" * 35)
    
    try:
        from src.rag.s3.models import S3Config
        from src.rag.s3.s3_pipeline import S3Pipeline
        from src.rag.s3.gbr_reward import NaiveRAG
        
        # Create components
        retriever = MockRetriever()
        generator = MockGenerator()
        
        # Test query
        test_query = "What are the main uses of Python programming?"
        
        print(f"Test query: {test_query}")
        print()
        
        # Test baseline RAG
        print("üìä Baseline RAG Performance:")
        baseline_start = asyncio.get_event_loop().time()
        baseline_result = await NaiveRAG(retriever, generator).retrieve_and_generate(test_query)
        baseline_time = asyncio.get_event_loop().time() - baseline_start
        
        print(f"  Time: {baseline_time:.3f}s")
        print(f"  Documents: {baseline_result['num_documents']}")
        print(f"  Response: {baseline_result['response'][:100]}...")
        print()
        
        # Test s3 RAG
        print("üöÄ S3 RAG Performance:")
        config = S3Config(max_search_iterations=3)
        s3_pipeline = S3Pipeline(config, retriever, generator, [NaiveRAG(retriever, generator)])
        
        s3_result = await s3_pipeline.query(test_query, return_details=True)
        
        if s3_result.success:
            print(f"  Time: {s3_result.total_time:.3f}s")
            print(f"  Search time: {s3_result.search_time:.3f}s")
            print(f"  Generation time: {s3_result.generation_time:.3f}s")
            print(f"  Documents: {s3_result.search_state.total_documents}")
            print(f"  Iterations: {s3_result.search_state.iteration}")
            print(f"  Response: {s3_result.response[:100]}...")
            
            if s3_result.gbr_reward:
                print(f"  GBR Gain: {s3_result.gbr_reward.normalized_gain:.3f}")
                print(f"  Baseline score: {s3_result.gbr_reward.baseline_score:.3f}")
                print(f"  S3 score: {s3_result.gbr_reward.s3_score:.3f}")
                
                improvement = ((s3_result.gbr_reward.s3_score - s3_result.gbr_reward.baseline_score) / 
                             max(s3_result.gbr_reward.baseline_score, 0.001)) * 100
                print(f"  Improvement: {improvement:.1f}%")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Comparison failed: {e}")
        return False


async def main():
    """Run all s3 RAG demonstrations"""
    print("üöÄ PyGent Factory S3 RAG Framework Examples")
    print("=" * 50)
    print("Demonstrating s3 RAG: Superior performance with 90% less training data")
    print()
    
    demos = [
        ("Basic S3 RAG", demonstrate_s3_basic),
        ("S3 Training", demonstrate_s3_training),
        ("S3 vs Baseline", demonstrate_s3_comparison)
    ]
    
    passed = 0
    total = len(demos)
    
    for demo_name, demo_func in demos:
        print(f"Running {demo_name}...")
        try:
            success = await demo_func()
            if success:
                passed += 1
                print(f"‚úÖ {demo_name} completed successfully")
            else:
                print(f"‚ö†Ô∏è {demo_name} completed with issues")
        except Exception as e:
            print(f"‚ùå {demo_name} failed: {e}")
        print()
    
    print(f"üìà Results: {passed}/{total} demonstrations successful")
    
    if passed == total:
        print("\nüéâ All S3 RAG demonstrations completed!")
        print("\nüí° Key S3 RAG Benefits Demonstrated:")
        print("1. Iterative search refinement")
        print("2. Gain Beyond RAG (GBR) reward signals")
        print("3. Minimal training data requirements")
        print("4. Superior performance vs baseline RAG")
        print("5. Adaptive search strategies")
        
        print("\nüîß Next Steps:")
        print("1. Integrate with real embedding models")
        print("2. Scale to larger document collections")
        print("3. Implement full RL training pipeline")
        print("4. Deploy in production RAG systems")
    else:
        print("\n‚ö†Ô∏è Some demonstrations had issues")
        print("This is expected in a demo environment")


if __name__ == "__main__":
    print("üìã Prerequisites:")
    print("- PyGent Factory environment")
    print("- S3 RAG framework components")
    print("- Mock retriever and generator for demo")
    print()
    
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è Demo interrupted by user")
    except Exception as e:
        print(f"\n‚ùå Demo failed: {e}")
        logger.exception("Demo execution failed")
