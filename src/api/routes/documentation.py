"""
Documentation API Routes

Provides REST API endpoints for accessing PyGent Factory documentation,
including markdown files, navigation data, and search functionality,
persistent document creation/management with research agent integration.
"""

import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
from fastapi import APIRouter, HTTPException, Query, Depends
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
import markdown
from datetime import datetime
from sqlalchemy.orm import Session

# Import database models and session
from ...database.models import (
    DocumentationFile, DocumentationVersion,
    ResearchSession, DocumentationWorkflow
)
from ...database.connection import get_database_session
from ...security.auth import get_current_user, User
from ...services.document_service import get_document_service, DocumentService

logger = logging.getLogger(__name__)

router = APIRouter()

# Documentation configuration
DOCS_BASE_PATH = Path("docs")
ALLOWED_EXTENSIONS = {'.md', '.markdown'}


# Pydantic models for API requests/responses
class DocumentCreationRequest(BaseModel):
    title: str = Field(..., description="Document title")
    content: str = Field(..., description="Document content in Markdown")
    category: str = Field(..., description="Document category")
    file_path: str = Field(..., description="File path for the document")
    research_session_id: Optional[str] = Field(None, description="Associated research session ID")
    agent_workflow_id: Optional[str] = Field(None, description="Agent workflow ID if generated by agent")
    tags: Optional[List[str]] = Field(default_factory=list, description="Document tags")


class DocumentUpdateRequest(BaseModel):
    title: Optional[str] = Field(None, description="Updated document title")
    content: Optional[str] = Field(None, description="Updated document content")
    category: Optional[str] = Field(None, description="Updated document category")
    change_summary: Optional[str] = Field(None, description="Summary of changes made")
    tags: Optional[List[str]] = Field(None, description="Updated document tags")


class ResearchSessionRequest(BaseModel):
    title: str = Field(..., description="Research session title")
    description: Optional[str] = Field(None, description="Research session description")
    research_query: str = Field(..., description="Research query or objective")
    research_scope: Optional[Dict[str, Any]] = Field(default_factory=dict, description="Research scope and constraints")
    agent_config: Optional[Dict[str, Any]] = Field(default_factory=dict, description="Agent configuration")


class DocumentWorkflowRequest(BaseModel):
    workflow_type: str = Field(..., description="Type of workflow: creation, update, reorganization")
    title: str = Field(..., description="Workflow title")
    description: Optional[str] = Field(None, description="Workflow description")
    parameters: Optional[Dict[str, Any]] = Field(default_factory=dict, description="Workflow parameters")


def get_documentation_files() -> List[Dict[str, Any]]:
    """Get list of all documentation files with metadata."""
    docs_files = []
    
    if not DOCS_BASE_PATH.exists():
        logger.warning(f"Documentation directory not found: {DOCS_BASE_PATH}")
        return docs_files
    
    for file_path in DOCS_BASE_PATH.rglob("*"):
        if file_path.is_file() and file_path.suffix.lower() in ALLOWED_EXTENSIONS:
            try:
                stat = file_path.stat()
                relative_path = file_path.relative_to(DOCS_BASE_PATH)
                
                # Extract title from filename or first line
                title = file_path.stem.replace('_', ' ').replace('-', ' ').title()
                  # Try to extract title from first line if it's a header
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        first_line = f.readline().strip()
                        if first_line.startswith('#'):
                            title = first_line.lstrip('# ').strip()
                except Exception:
                    pass
                
                docs_files.append({
                    'id': str(relative_path).replace('\\', '/'),
                    'title': title,
                    'path': str(relative_path).replace('\\', '/'),
                    'size': stat.st_size,
                    'modified': datetime.fromtimestamp(stat.st_mtime).isoformat(),
                    'category': get_file_category(file_path.name)
                })
            except Exception as e:
                logger.error(f"Error processing doc file {file_path}: {e}")
                continue
    
    return sorted(docs_files, key=lambda x: x['title'])


def get_file_category(filename: str) -> str:
    """Determine category based on filename patterns."""
    filename_lower = filename.lower()
    
    if any(keyword in filename_lower for keyword in ['a2a', 'agent', 'protocol']):
        return 'A2A Protocol'
    elif any(keyword in filename_lower for keyword in ['dgm', 'darwinian', 'manufacturing']):
        return 'DGM System'
    elif any(keyword in filename_lower for keyword in ['security', 'auth', 'risk']):
        return 'Security'
    elif any(keyword in filename_lower for keyword in ['implementation', 'plan', 'roadmap']):
        return 'Implementation'
    elif any(keyword in filename_lower for keyword in ['architecture', 'system', 'overview']):
        return 'Architecture'
    elif filename_lower.startswith('master'):
        return 'Master Documentation'
    else:
        return 'General'


def get_documentation_navigation() -> Dict[str, Any]:
    """Generate navigation structure for documentation."""
    files = get_documentation_files()
    
    # Group by category
    categories = {}
    for file_data in files:
        category = file_data['category']
        if category not in categories:
            categories[category] = []
        categories[category].append(file_data)
    
    return {
        'categories': categories,
        'total_files': len(files),
        'last_updated': datetime.now().isoformat()
    }


@router.get("/")
async def get_documentation_index():
    """Get documentation index with navigation structure."""
    try:
        navigation = get_documentation_navigation()
        return JSONResponse(content={
            'status': 'success',
            'data': navigation
        })
    except Exception as e:
        logger.error(f"Error getting documentation index: {e}")
        raise HTTPException(status_code=500, detail="Failed to load documentation index")


@router.get("/files")
async def list_documentation_files(
    category: Optional[str] = Query(None, description="Filter by category"),
    search: Optional[str] = Query(None, description="Search in titles")
):
    """List all documentation files with optional filtering."""
    try:
        files = get_documentation_files()
        
        # Apply category filter
        if category:
            files = [f for f in files if f['category'].lower() == category.lower()]
        
        # Apply search filter
        if search:
            search_lower = search.lower()
            files = [f for f in files if search_lower in f['title'].lower()]
        
        return JSONResponse(content={
            'status': 'success',
            'data': {
                'files': files,
                'total': len(files)
            }
        })
    except Exception as e:
        logger.error(f"Error listing documentation files: {e}")
        raise HTTPException(status_code=500, detail="Failed to list documentation files")


@router.get("/file/{file_path:path}")
async def get_documentation_file(file_path: str):
    """Get a specific documentation file content."""
    try:
        # Sanitize the file path
        safe_path = Path(file_path).as_posix()
        full_path = DOCS_BASE_PATH / safe_path
        
        # Security check - ensure the path is within docs directory
        try:
            full_path.resolve().relative_to(DOCS_BASE_PATH.resolve())
        except ValueError:
            raise HTTPException(status_code=400, detail="Invalid file path")
        
        if not full_path.exists():
            raise HTTPException(status_code=404, detail="Documentation file not found")
        
        if not full_path.is_file() or full_path.suffix.lower() not in ALLOWED_EXTENSIONS:
            raise HTTPException(status_code=400, detail="Invalid file type")
        
        # Read and process the file
        with open(full_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Convert markdown to HTML
        html_content = markdown.markdown(
            content, 
            extensions=['toc', 'codehilite', 'fenced_code', 'tables']
        )
        
        # Extract metadata
        stat = full_path.stat()
        
        return JSONResponse(content={
            'status': 'success',
            'data': {
                'path': safe_path,
                'title': full_path.stem.replace('_', ' ').replace('-', ' ').title(),
                'content': content,
                'html_content': html_content,
                'size': stat.st_size,
                'modified': datetime.fromtimestamp(stat.st_mtime).isoformat(),
                'category': get_file_category(full_path.name)
            }
        })
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error reading documentation file {file_path}: {e}")
        raise HTTPException(status_code=500, detail="Failed to read documentation file")


@router.get("/search")
async def search_documentation(
    query: str = Query(..., description="Search query"),
    limit: int = Query(20, description="Maximum number of results")
):
    """Search documentation content."""
    try:
        if not query.strip():
            return JSONResponse(content={
                'status': 'success',
                'data': {
                    'results': [],
                    'total': 0,
                    'query': query
                }
            })
        
        results = []
        query_lower = query.lower()
        
        for file_path in DOCS_BASE_PATH.rglob("*"):
            if file_path.is_file() and file_path.suffix.lower() in ALLOWED_EXTENSIONS:
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # Search in content
                    if query_lower in content.lower():
                        relative_path = file_path.relative_to(DOCS_BASE_PATH)
                        
                        # Find context around matches
                        lines = content.split('\n')
                        matching_lines = []
                        for i, line in enumerate(lines):
                            if query_lower in line.lower():
                                # Get context (line before, match, line after)
                                context_start = max(0, i - 1)
                                context_end = min(len(lines), i + 2)
                                context = ' '.join(lines[context_start:context_end])
                                matching_lines.append({
                                    'line_number': i + 1,
                                    'context': context[:200] + '...' if len(context) > 200 else context
                                })
                        
                        results.append({
                            'path': str(relative_path).replace('\\', '/'),
                            'title': file_path.stem.replace('_', ' ').replace('-', ' ').title(),
                            'category': get_file_category(file_path.name),
                            'matches': matching_lines[:3]  # Limit matches per file
                        })
                        
                        if len(results) >= limit:
                            break
                            
                except Exception as e:
                    logger.error(f"Error searching in file {file_path}: {e}")
                    continue
        
        return JSONResponse(content={
            'status': 'success',
            'data': {
                'results': results,
                'total': len(results),
                'query': query
            }
        })
    except Exception as e:
        logger.error(f"Error searching documentation: {e}")
        raise HTTPException(status_code=500, detail="Failed to search documentation")


@router.get("/categories")
async def get_documentation_categories():
    """Get list of all documentation categories."""
    try:
        files = get_documentation_files()
        categories = {}
        
        for file_data in files:
            category = file_data['category']
            if category not in categories:
                categories[category] = 0
            categories[category] += 1
        
        return JSONResponse(content={
            'status': 'success',
            'data': {
                'categories': [
                    {'name': name, 'count': count} 
                    for name, count in sorted(categories.items())
                ]
            }
        })
    except Exception as e:
        logger.error(f"Error getting documentation categories: {e}")
        raise HTTPException(status_code=500, detail="Failed to get documentation categories")


@router.get("/stats")
async def get_documentation_stats():
    """Get documentation statistics."""
    try:
        files = get_documentation_files()
        
        total_size = sum(f['size'] for f in files)
        categories = {}
        for file_data in files:
            category = file_data['category']
            categories[category] = categories.get(category, 0) + 1
        
        return JSONResponse(content={
            'status': 'success',
            'data': {
                'total_files': len(files),
                'total_size_bytes': total_size,
                'total_size_mb': round(total_size / (1024 * 1024), 2),
                'categories': categories,
                'last_updated': max((f['modified'] for f in files), default=datetime.now().isoformat()) if files else None
            }
        })
    except Exception as e:
        logger.error(f"Error getting documentation stats: {e}")
        raise HTTPException(status_code=500, detail="Failed to get documentation statistics")


@router.post("/create")
async def create_document(
    request: DocumentCreationRequest,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_database_session),
    document_service: DocumentService = Depends(get_document_service)
):
    """Create a new document with persistent storage."""
    try:
        # Check if document already exists for this user
        existing_doc = db.query(DocumentationFile).filter(
            DocumentationFile.file_path == request.file_path,
            DocumentationFile.user_id == current_user.id
        ).first()
        
        if existing_doc:
            raise HTTPException(status_code=400, detail="Document already exists at this path")
          # Create document using DocumentService
        doc = document_service.create_documentation_file(
            user_id=current_user.id,
            title=request.title,
            content=request.content,
            file_path=request.file_path,
            category=request.category,
            research_session_id=request.research_session_id,
            agent_workflow_id=request.agent_workflow_id,
            tags=request.tags
        )
        # Also save to filesystem for backward compatibility
        try:
            full_path = DOCS_BASE_PATH / request.file_path
            full_path.parent.mkdir(parents=True, exist_ok=True)
            with open(full_path, 'w', encoding='utf-8') as f:
                f.write(request.content)
        except Exception as e:
            logger.warning(f"Failed to save to filesystem: {e}")
        
        return JSONResponse(content={
            'status': 'success',
            'data': {
                'id': doc.id,
                'file_path': doc.file_path,
                'title': doc.title,
                'created_at': doc.created_at.isoformat(),
                'user_id': current_user.id,
                'backend_stored': True,
                'filesystem_backup': True
            }
        })
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error creating document: {e}")
        raise HTTPException(status_code=500, detail="Failed to create document")


@router.put("/update/{doc_id}")
async def update_document(
    doc_id: str,
    request: DocumentUpdateRequest,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_database_session),
    document_service: DocumentService = Depends(get_document_service)
):
    """Update an existing document with version tracking."""
    try:        # Check ownership
        doc = document_service.get_documentation_file_by_id(doc_id, current_user.id)
        if not doc:
            raise HTTPException(status_code=404, detail="Document not found")
        
        # Update document using DocumentService
        updated_doc = document_service.update_documentation_file(
            user_id=current_user.id,
            document_id=doc_id,
            title=request.title,
            content=request.content,
            category=request.category,
            change_summary=request.change_summary,
            tags=request.tags
        )
        # Update filesystem backup
        if request.content:
            try:
                full_path = DOCS_BASE_PATH / doc.file_path
                full_path.parent.mkdir(parents=True, exist_ok=True)
                with open(full_path, 'w', encoding='utf-8') as f:
                    f.write(request.content)
            except Exception as e:
                logger.warning(f"Failed to update filesystem backup: {e}")
        
        return JSONResponse(content={
            'status': 'success',
            'data': {
                'id': updated_doc.id,
                'version': updated_doc.version_number,
                'updated_at': updated_doc.updated_at.isoformat(),
                'user_id': current_user.id,
                'backend_stored': True
            }
        })
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error updating document: {e}")
        raise HTTPException(status_code=500, detail="Failed to update document")


@router.post("/research-session")
async def create_research_session(
    request: ResearchSessionRequest,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_database_session)
):
    """Create a new research session for document generation."""
    try:
        session = ResearchSession(
            id=f"research_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{hash(request.research_query) % 10000}",
            user_id=current_user.id,
            title=request.title,
            description=request.description,
            research_query=request.research_query,
            research_scope=request.research_scope,
            agent_config=request.agent_config,
            session_type='research',
            status='active'
        )        
        db.add(session)
        db.commit()
        
        return JSONResponse(content={
            'status': 'success',
            'data': {
                'session_id': session.id,
                'title': session.title,
                'status': session.status,
                'user_id': current_user.id,
                'started_at': session.started_at.isoformat()
            }
        })
    except Exception as e:
        db.rollback()
        logger.error(f"Error creating research session: {e}")
        raise HTTPException(status_code=500, detail="Failed to create research session")


@router.post("/workflow")
async def create_documentation_workflow(
    request: DocumentWorkflowRequest,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_database_session)
):
    """Create a new documentation workflow."""
    try:
        workflow = DocumentationWorkflow(
            id=f"workflow_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{hash(request.title) % 10000}",
            user_id=current_user.id,
            workflow_type=request.workflow_type,
            title=request.title,
            description=request.description,
            parameters=request.parameters,
            status='pending'
        )        
        db.add(workflow)
        db.commit()
        
        return JSONResponse(content={
            'status': 'success',
            'data': {
                'workflow_id': workflow.id,
                'title': workflow.title,
                'status': workflow.status,
                'user_id': current_user.id,
                'created_at': workflow.created_at.isoformat()
            }
        })
    except Exception as e:
        db.rollback()
        logger.error(f"Error creating workflow: {e}")
        raise HTTPException(status_code=500, detail="Failed to create workflow")


@router.get("/persistent")
async def get_persistent_documents(
    category: Optional[str] = Query(None, description="Filter by category"),
    search: Optional[str] = Query(None, description="Search in titles and content"),
    limit: int = Query(50, description="Maximum number of results"),
    offset: int = Query(0, description="Offset for pagination"),
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_database_session),
    document_service: DocumentService = Depends(get_document_service)
):
    """Get documents from persistent storage with filtering and pagination."""
    try:        # Use DocumentService to get user's documents
        documents = document_service.list_documentation_files(
            user_id=current_user.id,
            category=category,
            search_query=search,
            limit=limit,
            offset=offset
        )        # Format response
        docs_data = []
        for doc in documents['documents']:
            docs_data.append({
                'id': doc.id,
                'file_path': doc.file_path,
                'title': doc.title,
                'category': doc.category,
                'size': doc.file_size,
                'created_at': doc.created_at.isoformat(),
                'updated_at': doc.updated_at.isoformat(),
                'generated_by_agent': doc.generated_by_agent,
                'research_session_id': doc.research_session_id,
                'backend_stored': True,
                'temporary_copy': False  # These are permanent backend documents
            })
        
        return JSONResponse(content={
            'status': 'success',
            'data': {
                'documents': docs_data,
                'total': documents['total'],
                'offset': offset,
                'limit': limit,
                'user_id': current_user.id,
                'storage_type': 'persistent_backend'
            }
        })
    except Exception as e:
        logger.error(f"Error getting persistent documents: {e}")
        raise HTTPException(status_code=500, detail="Failed to get persistent documents")


@router.get("/persistent/{doc_id}")
async def get_persistent_document(
    doc_id: str,
    version: Optional[int] = Query(None, description="Specific version number"),
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_database_session),
    document_service: DocumentService = Depends(get_document_service)
):
    """Get a specific document from persistent storage."""
    try:        # Check ownership using DocumentService
        doc = document_service.get_documentation_file_by_id(doc_id, current_user.id)
        if not doc:
            raise HTTPException(status_code=404, detail="Document not found")
        
        # Get specific version if requested
        content = doc.content
        if version:
            version_obj = db.query(DocumentationVersion).filter(
                DocumentationVersion.documentation_file_id == doc_id,
                DocumentationVersion.version_number == version
            ).first()
            if version_obj:
                content = version_obj.content
        
        # Get tags
        tags = [tag.tag_name for tag in doc.tags]
        
        # Get versions
        versions = db.query(DocumentationVersion).filter(
            DocumentationVersion.documentation_file_id == doc_id
        ).order_by(DocumentationVersion.version_number.desc()).all()
        
        version_list = [{
            'version_number': v.version_number,
            'created_at': v.created_at.isoformat(),
            'change_summary': v.change_summary,
            'created_by': v.created_by
        } for v in versions]
        
        return JSONResponse(content={
            'status': 'success',
            'data': {
                'id': doc.id,
                'file_path': doc.file_path,
                'title': doc.title,
                'category': doc.category,
                'content': content,
                'created_at': doc.created_at.isoformat(),
                'updated_at': doc.updated_at.isoformat(),
                'tags': tags,
                'versions': version_list,
                'current_version': version or len(version_list),
                'user_id': current_user.id,
                'backend_stored': True,
                'temporary_copy': False
            }
        })
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting persistent document: {e}")
        raise HTTPException(status_code=500, detail="Failed to get persistent document")


@router.get("/research-sessions")
async def get_research_sessions(
    status: Optional[str] = Query(None, description="Filter by status"),
    limit: int = Query(20, description="Maximum number of results"),
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_database_session)
):
    """Get research sessions for the current user."""
    try:
        query = db.query(ResearchSession).filter(ResearchSession.user_id == current_user.id)
        
        if status:
            query = query.filter(ResearchSession.status == status)
        
        sessions = query.order_by(ResearchSession.started_at.desc()).limit(limit).all()
        
        sessions_data = []
        for session in sessions:
            sessions_data.append({
                'id': session.id,
                'title': session.title,
                'description': session.description,
                'status': session.status,
                'documents_generated': session.documents_generated,
                'started_at': session.started_at.isoformat(),
                'completed_at': session.completed_at.isoformat() if session.completed_at else None,
                'research_query': session.research_query,
                'user_id': current_user.id
            })
        
        return JSONResponse(content={
            'status': 'success',
            'data': {
                'sessions': sessions_data,
                'total': len(sessions_data),
                'user_id': current_user.id
            }
        })
    except Exception as e:
        logger.error(f"Error getting research sessions: {e}")
        raise HTTPException(status_code=500, detail="Failed to get research sessions")
