# Multi-stage GPU-optimized Dockerfile leveraging latest BuildKit features
# Optimized for NVIDIA RTX 3080 with CUDA 12.9

# Build arguments
ARG CUDA_VERSION=12.9
ARG PYTHON_VERSION=3.11
ARG TORCH_CUDA_ARCH_LIST=8.6
ARG BUILD_TYPE=gpu
ARG ENABLE_OPTIMIZATIONS=true

# Base Ubuntu image (we'll install CUDA components as needed)
FROM ubuntu:22.04 AS cuda-base

# Install system dependencies with cache mount
RUN --mount=type=cache,target=/var/cache/apt,sharing=locked \
    --mount=type=cache,target=/var/lib/apt,sharing=locked \
    apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    cmake \
    curl \
    git \
    wget \
    ca-certificates \
    libjpeg-dev \
    libpng-dev \
    libgl1-mesa-glx \
    libglib2.0-0 \
    libsm6 \
    libxext6 \
    libxrender-dev \
    libgomp1 \
    && rm -rf /var/lib/apt/lists/*

# Python installation stage
FROM cuda-base AS python-base

# Install Python with cache mount
RUN --mount=type=cache,target=/var/cache/apt,sharing=locked \
    --mount=type=cache,target=/var/lib/apt,sharing=locked \
    apt-get update && apt-get install -y --no-install-recommends \
    python3 \
    python3-dev \
    python3-pip \
    python3-distutils \
    && ln -sf /usr/bin/python3 /usr/bin/python

# Upgrade pip with cache mount
RUN --mount=type=cache,target=/root/.cache/pip \
    python3 -m pip install --upgrade pip setuptools wheel

# Dependencies installation stage
FROM python-base AS deps-installer

# Set working directory
WORKDIR /app

# Copy dependency files first for better caching
COPY requirements.txt requirements-gpu.txt ./

# Install Python dependencies with cache mounts
RUN --mount=type=cache,target=/root/.cache/pip \
    --mount=type=cache,target=/root/.cache/torch \
    --mount=type=secret,id=github_token,env=GITHUB_TOKEN \
    pip install --no-deps torch torchvision torchaudio \
    --index-url https://download.pytorch.org/whl/cu121 && \
    pip install -r requirements.txt && \
    pip install -r requirements-gpu.txt

# Install additional GPU-optimized packages
RUN --mount=type=cache,target=/root/.cache/pip \
    pip install \
    nvidia-ml-py3 \
    cupy-cuda12x \
    numba[cuda] \
    tensorrt \
    onnxruntime-gpu \
    triton \
    flash-attn \
    xformers

# Development dependencies (conditional)
ARG BUILD_TYPE
RUN --mount=type=cache,target=/root/.cache/pip \
    if [ "$BUILD_TYPE" = "dev" ]; then \
        pip install \
        jupyter \
        tensorboard \
        wandb \
        pytest \
        black \
        isort \
        mypy; \
    fi

# Application build stage
FROM deps-installer AS app-builder

# Copy source code with bind mount for efficiency
RUN --mount=type=bind,source=.,target=/src,rw \
    --mount=type=cache,target=/app/.git \
    cp -r /src/src /app/ && \
    cp -r /src/configs /app/ && \
    cp -r /src/scripts /app/ && \
    if [ -d "/src/models" ]; then cp -r /src/models /app/; fi

# Compile Python files for performance
RUN python -m compileall /app/src

# Model preparation stage (optional)
FROM app-builder AS model-stage

# Download and prepare models with cache mount
RUN --mount=type=cache,target=/root/.cache/huggingface \
    --mount=type=secret,id=huggingface_token,env=HUGGINGFACE_HUB_TOKEN \
    --mount=type=bind,source=scripts,target=/scripts,ro \
    if [ -f "/scripts/download_models.py" ]; then \
        python /scripts/download_models.py; \
    fi

# Production runtime stage
FROM python-base AS production

# Create non-root user for security
RUN groupadd -r pygent && useradd -r -g pygent -s /bin/bash pygent

# Set working directory
WORKDIR /app

# Copy installed packages from deps stage
COPY --from=deps-installer /usr/local/lib/python${PYTHON_VERSION}/site-packages /usr/local/lib/python${PYTHON_VERSION}/site-packages
COPY --from=deps-installer /usr/local/bin /usr/local/bin

# Copy application code
COPY --from=app-builder --chown=pygent:pygent /app /app

# Copy models (if available)
COPY --from=model-stage --chown=pygent:pygent /app/models /app/models

# Create necessary directories
RUN mkdir -p /app/.cuda-cache /app/.numba-cache /app/logs /app/tmp && \
    chown -R pygent:pygent /app

# Set environment variables for GPU optimization
ENV NVIDIA_VISIBLE_DEVICES=all \
    NVIDIA_DRIVER_CAPABILITIES=compute,utility \
    CUDA_VISIBLE_DEVICES=0 \
    CUDA_DEVICE_ORDER=PCI_BUS_ID \
    CUDA_CACHE_PATH=/app/.cuda-cache \
    NUMBA_CACHE_DIR=/app/.numba-cache \
    TORCH_CUDA_ARCH_LIST=${TORCH_CUDA_ARCH_LIST} \
    TORCH_ALLOW_TF32_CUBLAS_OVERRIDE=1 \
    TORCH_CUDNN_V8_API_ENABLED=1 \
    PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512 \
    OMP_NUM_THREADS=8 \
    MKL_NUM_THREADS=8 \
    PYTHONPATH=/app/src \
    PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD python -c "import torch; print('CUDA available:', torch.cuda.is_available()); exit(0 if torch.cuda.is_available() else 1)"

# Switch to non-root user
USER pygent

# Expose ports
EXPOSE 8000 8001 8002 6006

# Default command
CMD ["python", "-m", "src.main"]

# Development stage (extends production)
FROM production AS development

# Switch back to root for development tools
USER root

# Install development dependencies
RUN --mount=type=cache,target=/root/.cache/pip \
    pip install \
    jupyter \
    tensorboard \
    wandb \
    pytest \
    black \
    isort \
    mypy \
    debugpy

# Install additional development tools
RUN --mount=type=cache,target=/var/cache/apt,sharing=locked \
    --mount=type=cache,target=/var/lib/apt,sharing=locked \
    apt-get update && apt-get install -y --no-install-recommends \
    vim \
    htop \
    nvtop \
    tmux \
    && rm -rf /var/lib/apt/lists/*

# Create development directories
RUN mkdir -p /app/notebooks /app/experiments && \
    chown -R pygent:pygent /app

# Switch back to non-root user
USER pygent

# Development command
CMD ["jupyter", "lab", "--ip=0.0.0.0", "--port=8888", "--no-browser", "--allow-root"]

# Testing stage
FROM development AS testing

# Copy test files
COPY --chown=pygent:pygent tests/ /app/tests/

# Run tests
RUN python -m pytest /app/tests/ -v --tb=short

# Benchmark stage for performance testing
FROM production AS benchmark

# Install benchmarking tools
USER root
RUN --mount=type=cache,target=/root/.cache/pip \
    pip install \
    memory-profiler \
    py-spy \
    torch-tb-profiler

USER pygent

# Benchmark command
CMD ["python", "-m", "src.benchmark"]
